# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

The dataset contains UCI Bank Marketing client details including their marital status, age, housing, education etc.
The goal is to train a binary classification model as we want to predict if a client will subscribe to a term deposit with the bank.
I generated models through two main processes: AzureML HyperDrive and Azure Automated Machine Learning (AutoML).
I generated 32 LogisticRegression models by HyperDrive. The best performing model had an accuracy of 0.9162367.
The best model generated by AutoML was a VotingEnsemble with an accuracy of 0.91769.


## Scikit-learn Pipeline

The pipeline contains 3 main parts: dataset, training script train.py, and the Jupyter Notebook which contains the pipeline.
The data is loaded as TabularDataset via TabularDatasetFactory. Next, the data is cleaned in the clean_data function in train.py. The cleaning process includes one-hot encoding of months, weekdays and other categorical fields. It also contains binary encoding of fields such as marital, housing, loan and poutcome. 
After cleaning, the data is split into training and testing sets with ratio 80/20 respectively.
The LogisticRegression model from the scikit-learn lib is created with hyperparameters Regularization Strength ('C') and Max Iterations ('max_iter'), and trained with the training set. In order to optimize these hyperparameters, the RandomParameterSampling sampler is used which randomly selects values for 'C' and 'max_iter' hyperparameters for each run. For 'C' the value is between 0.001 and 100.0 while the value for 'max_iter' is either 10, 50, 100, 500 or 1000. By using random sampling, multiple models can be trained and tested with a variety of different hyperparameters. With that approach, the best and a high-performing model can be found without using other more resource intensive samplers like e.g. grid sampling.
For early termination we use a BanditPolicy policy with a slack_factor of 0.1 and evaluation_interval of 1. The policy allows for the termination of any model with accuracy below (1 / (1 + 0.1)) (91%) of the best performing run. That allows to terminate poor-performing runs and save resources.


## AutoML
AutoML resulted in 35 runs using different algorithms and hyperparameters. The best performing run was using Votingnsemble with a hyperparameter which was algorithm : ['XGBoostClassifier', 'LightGBM', 'LightGBM', 'XGBoostClassifier', 'XGBoostClassifier', 'RandomForest']. The best model achieved accuracy of 0.91769.


## Pipeline comparison

The best models from HyperDrive run (Logistic Regression) and AzureML runs where very similar with difference in accuracy of 0.0014533 for better performing model by Azure AutoML.
The HyperDrive took 17.1 minutes for 32 runs (limited to 32 as per the configuration) and the AutoML run took 57.16 minutes and generated 36 models.
The HyperDrive gives undabutelly more control over the run and configuration but it is more complex to implement. The AutoML is quick and easy to code and configure and gives a slightly better performing model. Additionally, the AutoML model gives explanations and various plots allowing to better understand the selected model e.g. top features by their importance.


## Future work

I think one of the improvements to consider would be allowing more than 30 minutes for 'experiment_timeout_minutes' parameter in the AutoML config. That would result in more time for training models and better combination of hyperparameters which in turn would allow to get a model with higher accuracy.
Also, it seems that the source data could be more balanced to better train the models. Currently, based on the confusion matrix of the best performing AutoML model, it contains 28019 entries of true negatives and only 2219 of true positives.
